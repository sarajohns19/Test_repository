{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science, CS 5963 / Math 3900\n",
    "*CS 5963 / MATH 3900, University of Utah, http://datasciencecourse.net/* \n",
    "\n",
    "## Lecture 9: Decision Trees\n",
    "\n",
    "\n",
    "We continue discussing classification problems and introduce the decision trees method. In the process, we introduce the Python library [scikit-learn](http://scikit-learn.org/). But first, we'll recap some fundamental concepts in supervised learning. \n",
    "\n",
    "Recommended Reading: ISLR, Ch. 8.; Grus, Ch. 17.\n",
    "\n",
    "This Lecture is in part based on [this lab](https://github.com/cs109/2015lab7/blob/master/Lab7-Botany%20and%20Ensemble%20Methods.ipynb) and [this blog](http://hamelg.blogspot.com/2015/11/python-for-data-analysis-part-29.html), as well as on the [scikit learn documentation](http://scikit-learn.org/stable/modules/tree.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "What is supervised learning? \n",
    "\n",
    "Supervised learning is a machine learning technique that uses labeled training data to generate a model. The input is a feature vector plus a label. \n",
    "\n",
    "We'll be using [a dataset of Titanic passengers](https://www.kaggle.com/c/titanic/) in this lab, and we'll try to develop a model to see whether a particular passenger will survive or not. This is how [the dataset](titanic.csv) looks like:\n",
    "\n",
    "\n",
    "| PassengerId | Survived | Pclass | Name                                                                               | Sex    | Age  | SibSp | Parch | Ticket             | Fare     | Cabin           | Embarked |\n",
    "|-------------|----------|--------|------------------------------------------------------------------------------------|--------|------|-------|-------|--------------------|----------|-----------------|----------|\n",
    "| 1           | 0        | 3      | Braund, Mr. Owen Harris                                                            | male   | 22   | 1     | 0     | A/5 21171          | 7.25     |                 | S        |\n",
    "| 2           | 1        | 1      | Cumings, Mrs. John Bradley (Florence Briggs Thayer)                                | female | 38   | 1     | 0     | PC 17599           | 71.2833  | C85             | C        |\n",
    "| 3           | 1        | 3      | Heikkinen, Miss. Laina                                                             | female | 26   | 0     | 0     | STON/O2. 3101282   | 7.925    |                 | S        |\n",
    "| 4           | 1        | 1      | Futrelle, Mrs. Jacques Heath (Lily May Peel)                                       | female | 35   | 1     | 0     | 113803             | 53.1     | C123            | S        |\n",
    "| 5           | 0        | 3      | Allen, Mr. William Henry                                                           | male   | 35   | 0     | 0     | 373450             | 8.05     |                 | S        |\n",
    "\n",
    "\n",
    "Here are the variable descriptions for the non-obvious variables:\n",
    " * **survival:** Survival (0 = No; 1 = Yes)\n",
    " * **pclass:**          Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    " * **sibsp:**           Number of Siblings/Spouses Aboard\n",
    " * **parch:**           Number of Parents/Children Aboard\n",
    " * **ticket:**          Ticket Number\n",
    " * **fare:**            Passenger Fare\n",
    " * **cabin:**           Cabin\n",
    " * **embarked:**        Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "\n",
    "We're trying to decide whether a passenger with particular attributes will survive the Titanic disaster, so the **survival** flag is our label, the other columns are the **features**. \n",
    "\n",
    "We will **generate a model based on labeled data (hence \"supervised learning\")**, and ideally this model **should also work for new, similar data**. \n",
    "\n",
    "If a model satisfies this criterion, we say that it is **generalizable**. \n",
    "\n",
    "If a model has 100% accuracy on the training dataset but doesn't generalize to new data, then it isn't a very good model. We say that this model has **overfit** the data. \n",
    "\n",
    "The opposite of overfitting is **underfitting**, which happens when the model isn't complex enough to have good accuracy on the training dataset. \n",
    "\n",
    "**Cross-validation** is a general method for assessing how the results of a model (classification, regression,...) will *generalize* to an independent data set. \n",
    "\n",
    "In classification, cross-validation is a method for assessing how well the classification model will predict the class of points that weren't used to *train* the model. \n",
    "\n",
    "The idea of the method is simple: \n",
    "1. Split the dataset into two groups: the training dataset and the testing dataset. \n",
    "+ Train the model on the training dataset. \n",
    "+ Check the accuracy of the model on the testing dataset. \n",
    "\n",
    "In practice, you have to decide how to split the data into groups (i.e. how large the groups should be). You might also want to repeat the experiment so that the assessment doesn't depend on the way in which you split the data into groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Decision trees are very intuitive classification and regression tools. Consider the following tree:\n",
    "\n",
    "![Titanic Decision Tree](titanic_tree.png)\n",
    "\n",
    "[This tree](https://en.wikipedia.org/wiki/Decision_tree_learning) predicts the survival of passengers on the titanic.  The figures under the leaves show the probability of survival and the percentage of observations in the leaf.\n",
    "\n",
    "The use of a decision tree is very simple, say someone gives you this tree and a new person. In order to predict whether or not the person would have died on the titanic, you ask the following questions, in order:\n",
    "\n",
    " * Is the person male? If no, we predict they would have survived. If yes, continue.\n",
    " * Is the person older than 9.5 years? If yes, we predict they would have died. If no, continue.\n",
    " * Did the person have more than 3 or more siblings? If yes, we predict they would have died. If no, they would have survived\n",
    "\n",
    "The question we'll move to now is how would one build such a tree? \n",
    "\n",
    "There is [a very good, interactive tutorial online](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/), we'll explore this tutorial in the next couple of minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Decision Tree\n",
    "\n",
    "Building a decision tree isn't really much harder than reading one, here's the essential rundown on the example of the ID3 algorithm:\n",
    "\n",
    " * If the data all have the same label, create a leaf node that predicts that label, and you're done.\n",
    " * If the list of attributes is empty (e.g., because you have used all already), create a leaf node that predicts the most common label.\n",
    " * Else, partition the data by each of the attributes; choose the partition with the lowest error.\n",
    " * Recursively continue on each partitioned subset using the remaining attributes.\n",
    " * Terminate when no attributes are left (see above), or when desired depth or another termination criterion  is reached.\n",
    " \n",
    "So, how do you choose the partition with the lowest error? There are various approaches.\n",
    "\n",
    "Let's say we're building a classification tree by considering a list of predictors. In our example above we want to be able to classify whether people have survived based on things like gender, age, the booked fare, etc. Let's call the variables $X_{i1}, X_{i2}, ..., X_{ip}$ ($i$ for passengers, $p$ for predictors). Initially, for the first split, we consider all the passengers and all the predictors. We also have an observed label $Y_i$ for each passenger. \n",
    "\n",
    "We first assign everyone to the same class, say $\\hat{Y}_i = 1$. We can calculate the squared error $Err = \\sum_i {(\\hat{Y}_i - Y_i)^2}$. We want to achieve two things: pick the **best split** for the **best predictor**.\n",
    "\n",
    "- At **each step** of the algorithm we consider a list of possible decision (or split), for example $X_{10} > 9$, i.e. age is greater than 9, or $X_{9} = female$.\n",
    "- For each possible decision we recalculate the predictor for that rule, for example $\\hat{Y}_i = 1$ if $X_{10} > 9$ and $0$ otherwise.\n",
    "- We recalculate the error for each possible decision: $Err = \\sum_i {(\\hat{Y}_i - Y_i)^2}$\n",
    "- We choose the decision that reduces the error by the largest amount.\n",
    "- Then continue with the next step on the reduced input set.\n",
    "\n",
    "An alternative measure is the **Gini impurity**, a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.\n",
    "\n",
    "This histogram nicely illustrates the split decision:\n",
    "\n",
    "![Split decision](split_decision.png)\n",
    "\n",
    "There are other criteria besides error or impurity to decide whether we want to continue to split. Mostly, these other criteria are used to avoid overfitting. \n",
    "\n",
    "For example, we could **limit the depth of a tree**, or  **only split when we have more than N samples left**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees with SciKit Learn\n",
    "\n",
    "SciKit learn has [a nice decision tree implementation](http://scikit-learn.org/stable/modules/tree.html) which we'll use to learn a tree for our Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = pd.read_csv(\"titanic.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some cleanup: \n",
    " * age has missing values, let's say wherever we have no value, we assume that the person is of mean age (this is not necessarily a good decision)\n",
    " * embarked has missing values, we add a dedicated category for unknown embarkation points\n",
    " * we need to convert the categorical values to numerical values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex   Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris    0  22.0      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  38.0      1      0   \n",
       "2                             Heikkinen, Miss. Laina    1  26.0      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  35.0      1      0   \n",
       "4                           Allen, Mr. William Henry    0  35.0      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin Embarked  \n",
       "0         A/5 21171   7.2500   NaN        S  \n",
       "1          PC 17599  71.2833   C85        C  \n",
       "2  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3            113803  53.1000  C123        S  \n",
       "4            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].mean())\n",
    "\n",
    "def sex_to_numeric(x):\n",
    "    if x=='male':\n",
    "        return 0\n",
    "    if x=='female':\n",
    "        return 1\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "titanic[\"Sex\"] = titanic[\"Sex\"].apply(sex_to_numeric)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>0</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>1</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>1</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name  Sex        Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    0  22.000000      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  38.000000      1   \n",
       "2                             Heikkinen, Miss. Laina    1  26.000000      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  35.000000      1   \n",
       "4                           Allen, Mr. William Henry    0  35.000000      0   \n",
       "5                                   Moran, Mr. James    0  29.699118      0   \n",
       "6                            McCarthy, Mr. Timothy J    0  54.000000      0   \n",
       "7                     Palsson, Master. Gosta Leonard    0   2.000000      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)    1  27.000000      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)    1  14.000000      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin  Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN         0  \n",
       "1      0          PC 17599  71.2833   C85         1  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN         0  \n",
       "3      0            113803  53.1000  C123         0  \n",
       "4      0            373450   8.0500   NaN         0  \n",
       "5      0            330877   8.4583   NaN         2  \n",
       "6      0             17463  51.8625   E46         0  \n",
       "7      1            349909  21.0750   NaN         0  \n",
       "8      2            347742  11.1333   NaN         0  \n",
       "9      0            237736  30.0708   NaN         1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will break if run more than once\n",
    "def embarked_to_numeric(x):\n",
    "    if x==\"S\":\n",
    "        return 0\n",
    "    if x==\"C\":\n",
    "        return 1\n",
    "    if x==\"Q\":\n",
    "        return 2\n",
    "    else: \n",
    "        return 3\n",
    "    \n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].apply(embarked_to_numeric)\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work only with the numerical and categorical variables and omit passengerID, Name, Ticket and Cabin. These values could contain some information, but it's hard to make sense of them without more context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex        Age  SibSp  Parch     Fare  Embarked\n",
       "0         0       3    0  22.000000      1      0   7.2500         0\n",
       "1         1       1    1  38.000000      1      0  71.2833         1\n",
       "2         1       3    1  26.000000      0      0   7.9250         0\n",
       "3         1       1    1  35.000000      1      0  53.1000         0\n",
       "4         0       3    0  35.000000      0      0   8.0500         0\n",
       "5         0       3    0  29.699118      0      0   8.4583         2\n",
       "6         0       1    0  54.000000      0      0  51.8625         0\n",
       "7         0       3    0   2.000000      3      1  21.0750         0\n",
       "8         1       3    1  27.000000      0      2  11.1333         0\n",
       "9         1       2    1  14.000000      1      0  30.0708         1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "titanic = titanic[features]\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7771a0b3ed0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitanic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Survived\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.pairplot(titanic, hue=\"Survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to find clear patterns here, except for the histogram that separates Sex.\n",
    "\n",
    "Here is some code that splits the data into training and test sets for cross-validation and selects features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels =[\"Survived\", \"Perished\"]\n",
    "\n",
    "def splitData(features):\n",
    "    titanic_predictors = titanic[features].as_matrix()\n",
    "    titanic_labels = titanic[\"Survived\"].as_matrix()\n",
    "\n",
    "    # Split into training and test sets\n",
    "    XTrain, XTest, yTrain, yTest = train_test_split(titanic_predictors, titanic_labels, random_state=1, test_size=0.5)\n",
    "    return XTrain, XTest, yTrain, yTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And more code for plotting decision trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display  \n",
    "import pydotplus \n",
    "from scipy import misc\n",
    "\n",
    "def renderTree(my_tree, features):\n",
    "    # hacky solution of writing to files and reading again\n",
    "    # necessary due to library bugs\n",
    "    filename = \"temp.dot\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f = tree.export_graphviz(my_tree, \n",
    "                                 out_file=f, \n",
    "                                 feature_names=features, \n",
    "                                 class_names=[\"Perished\", \"Survived\"],  \n",
    "                                 filled=True, \n",
    "                                 rounded=True,\n",
    "                                 special_characters=True)\n",
    "  \n",
    "    dot_data = \"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        dot_data = f.read()\n",
    "\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    image_name = \"temp.png\"\n",
    "    graph.write_png(image_name)  \n",
    "    display(Image(filename=image_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a decision tree that **ONLY operates on sex**! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decisionTree = tree.DecisionTreeClassifier()\n",
    "\n",
    "XTrain, XTest, yTrain, yTest = splitData([\"Sex\"])\n",
    "# fit the tree with the traing data\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "# predict with the training data\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "# measure accuracy\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "# predict with the test data\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "# measure accuracy\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n",
    "\n",
    "renderTree(decisionTree, [\"Sex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not bad, ~76% correct - sex seems to be a very good indicator of whether someone has survived or not.\n",
    "\n",
    "Let's try to add a couple of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  train tree on sex and the number of the number of siblings/spouses aboard\n",
    "used_features = [\"Sex\", \"SibSp\"]\n",
    "XTrain, XTest, yTrain, yTest = splitData(used_features)\n",
    "decisionTree = tree.DecisionTreeClassifier()\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n",
    "renderTree(decisionTree, used_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our accuracy on the training data has gone up, but the accuracy on the test data has gone down. It looks like we're overfitting. But maybe we just selected the wrong features? Let's just try all of them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "XTrain, XTest, yTrain, yTest = splitData(all_features)\n",
    "decisionTree = tree.DecisionTreeClassifier()\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n",
    "renderTree(decisionTree, all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "OK, clearly, we're overfitting the data - 98% accuracy on the training data and only ~75% on the test data. Yet, we've created a complicated tree. \n",
    "\n",
    "Decision trees are notorious for overfitting the data. There are two parameters that help us reign in overfitting:\n",
    "\n",
    "* **max_depth:** The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "* **min_samples_split:** The minimum number of samples required to split an internal node: If int, then consider min_samples_split as the minimum number. If float, then min_samples_split is a percentage and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "\n",
    "Let's try combinations of depth and min split size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decisionTree = tree.DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n",
    "renderTree(decisionTree, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decisionTree = tree.DecisionTreeClassifier(min_samples_split=15)\n",
    "\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n",
    "renderTree(decisionTree, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decisionTree = tree.DecisionTreeClassifier(max_depth=3, min_samples_split=10)\n",
    "\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n",
    "renderTree(decisionTree, all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like both, the minimum number of samples for splitting and the maximum depth help with overfitting and we achieve a 79-80% accuracy rate. That doesn't sound much better than just gender alone, but 4% improvement is a lot in classification. Also, in the last combination we have a pretty simple model. The main point seems to be:\n",
    "\n",
    " * Sex is the dominant factor at the root of the tree\n",
    " * For females, if you're in class 1 or 2 you're almost sure to survive\n",
    " * For males, if you were younger than 6.5 years old, you had a chance to survive. \n",
    " * Also note that there are branches that predict the same thing, but with different certainty. For example, in the male / adult category, if you were in \"first class\", you still were likely to die, but less likely than in second or third. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Decision Trees: Random Forests\n",
    "\n",
    "A great way to counteract overfitting is to combine multiple decision trees that were generated with some randomness and let them vote on the result. This approach is an *ensemble learning* technique, as it combines multiple learning algorithms. We're not going to go into details today, but you can read more [here](https://en.wikipedia.org/wiki/Random_forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
